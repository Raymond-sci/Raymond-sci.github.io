<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-08T23:59:34+01:00</updated><id>http://localhost:4000/feed.xml</id><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><entry><title type="html">cai2022eva</title><link href="http://localhost:4000/projects/cai2022eva" rel="alternate" type="text/html" title="cai2022eva" /><published>2022-10-06T00:00:00+01:00</published><updated>2022-10-06T00:00:00+01:00</updated><id>http://localhost:4000/projects/cai2022eva</id><content type="html" xml:base="http://localhost:4000/projects/cai2022eva"><![CDATA[<p><img src="/assets/project/cai2022eva/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Video activity localisation by natural language 
is an important yet challenging task, 
which aims to localise temporally a video segment that 
best corresponds to a query sentence
in an untrimmed (and often unstructured) video. 
Most of the existing methods address this task
in a fully supervised manner
to learn to localise <em>moment-of-interest</em> (MoI) in videos
according to their precise start and end time indices.
Considering the high annotation cost and subjective annotation bias,
recent works focus on weakly-supervised learning without per-sentence
temporal boundary annotations in training.</p>

<p>Existing weakly-supervised solutions
localise different MoIs individually, 
which is not optimal as it neglects the fact that
the cross-sentence relations in a paragraph 
play an important role in temporally localising multiple MoIs. 
Critically, an individual sentence is sometimes ambiguous 
out of its paragraph context
and the MoIs described by a paragraph are often
semantically related to each other in their corresponding sentences.</p>

<p>In this work,
we introduce a weakly-supervised method
for video activity localisation by natural language
called <em>Cross-sentence Relations Mining</em> (CRM).
The key idea is to
explore the cross-sentence relations in a paragraph 
as constraints to better interpret and match
complex moment-wise temporal and semantic relations in videos.
Specifically,
by assuming different activities in videos are described sequentially,
we formulate a <em>temporal consistency</em> constraint to encourage
the selected moments to be temporally ordered according to their
descriptions in a paragraph.
Moreover,
we encourage moment proposal selections to satisfy cross-sentence
broader semantics in context
to minimise video-text matching ambiguities.
To that end, we introduce a <em>semantic consistency</em> constraint
to ensure that a moment selected for any pairing of two 
sentences (concatenation) in a paragraph is consistent (overlapping) with
the union of the selected segments per sentence.</p>

<p>Our <strong>contributions</strong> are:
<strong>(1)</strong>
To our best knowledge, 
this is the first idea to develop a model using <em>cross-sentence relations</em>
in a paragraph to 
explicitly represent and compute <em>cross-moment relations</em> in videos,
so as to alleviate the ambiguity of each individual sentence in video activity localisation.
<strong>(2)</strong> 
We formulate a new weakly-supervised method 
for activity localisation by natural language 
called <em>Cross-sentence Relations Mining</em> (CRM), 
that trains a model with both temporal and semantic cross-sentence relations to improve
per-sentence temporal boundary prediction in testing.
<strong>(3)</strong>
Our approach achieves the state-of-the-art performance 
on two available activity localisation benchmarks,
especially so given more complex query descriptions.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Experiments were conducted on two challenging video activity localisation benchmarks
which demonstrate the compelling multi-modal understanding ability
of CRM over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li>
    <p><strong>Charades-STA</strong> contains 12,408/3720 video-query pairs from 5338/1334 videos for training and testing, respectively. The query sentences are composed of 7.2 words on average and the average duration of the MoIs and videos are 8.1 and 30.6 seconds.</p>
  </li>
  <li>
    <p><strong>ActivityNet-Captions</strong> is a larger-scale dataset composed of 19,290 videos with 37,417/17,505/17,031 MoIs in the train/val_1/val_2 split. The average length of queries is 14 words
while that of the MoIs and untrimmed videos are 36.2 and 117.6 seconds.</p>
  </li>
</ul>

<p>Please kindly refer to the <a href="http://www.eecs.qmul.ac.uk/~sgg/papers/CaiEtAl_BMVC2022.pdf">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2022scl</title><link href="http://localhost:4000/projects/huang2022scl" rel="alternate" type="text/html" title="huang2022scl" /><published>2022-10-06T00:00:00+01:00</published><updated>2022-10-06T00:00:00+01:00</updated><id>http://localhost:4000/projects/huang2022scl</id><content type="html" xml:base="http://localhost:4000/projects/huang2022scl"><![CDATA[<p><img src="/assets/project/huang2022scl/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Video activity localisation by natural language 
is an important yet challenging task, 
which aims to localise temporally a video segment that 
best corresponds to a query sentence
in an untrimmed (and often unstructured) video. 
Most of the existing methods address this task
in a fully supervised manner
to learn to localise <em>moment-of-interest</em> (MoI) in videos
according to their precise start and end time indices.
Considering the high annotation cost and subjective annotation bias,
recent works focus on weakly-supervised learning without per-sentence
temporal boundary annotations in training.</p>

<p>Existing weakly-supervised solutions
localise different MoIs individually, 
which is not optimal as it neglects the fact that
the cross-sentence relations in a paragraph 
play an important role in temporally localising multiple MoIs. 
Critically, an individual sentence is sometimes ambiguous 
out of its paragraph context
and the MoIs described by a paragraph are often
semantically related to each other in their corresponding sentences.</p>

<p>In this work,
we introduce a weakly-supervised method
for video activity localisation by natural language
called <em>Cross-sentence Relations Mining</em> (CRM).
The key idea is to
explore the cross-sentence relations in a paragraph 
as constraints to better interpret and match
complex moment-wise temporal and semantic relations in videos.
Specifically,
by assuming different activities in videos are described sequentially,
we formulate a <em>temporal consistency</em> constraint to encourage
the selected moments to be temporally ordered according to their
descriptions in a paragraph.
Moreover,
we encourage moment proposal selections to satisfy cross-sentence
broader semantics in context
to minimise video-text matching ambiguities.
To that end, we introduce a <em>semantic consistency</em> constraint
to ensure that a moment selected for any pairing of two 
sentences (concatenation) in a paragraph is consistent (overlapping) with
the union of the selected segments per sentence.</p>

<p>Our <strong>contributions</strong> are:
<strong>(1)</strong>
To our best knowledge, 
this is the first idea to develop a model using <em>cross-sentence relations</em>
in a paragraph to 
explicitly represent and compute <em>cross-moment relations</em> in videos,
so as to alleviate the ambiguity of each individual sentence in video activity localisation.
<strong>(2)</strong> 
We formulate a new weakly-supervised method 
for activity localisation by natural language 
called <em>Cross-sentence Relations Mining</em> (CRM), 
that trains a model with both temporal and semantic cross-sentence relations to improve
per-sentence temporal boundary prediction in testing.
<strong>(3)</strong>
Our approach achieves the state-of-the-art performance 
on two available activity localisation benchmarks,
especially so given more complex query descriptions.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Experiments were conducted on two challenging video activity localisation benchmarks
which demonstrate the compelling multi-modal understanding ability
of CRM over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li>
    <p><strong>Charades-STA</strong> contains 12,408/3720 video-query pairs from 5338/1334 videos for training and testing, respectively. The query sentences are composed of 7.2 words on average and the average duration of the MoIs and videos are 8.1 and 30.6 seconds.</p>
  </li>
  <li>
    <p><strong>ActivityNet-Captions</strong> is a larger-scale dataset composed of 19,290 videos with 37,417/17,505/17,031 MoIs in the train/val_1/val_2 split. The average length of queries is 14 words
while that of the MoIs and untrimmed videos are 36.2 and 117.6 seconds.</p>
  </li>
</ul>

<p>Please kindly refer to the <a href="https://arxiv.org/abs/2103.02662">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2022emb</title><link href="http://localhost:4000/projects/huang2022emb" rel="alternate" type="text/html" title="huang2022emb" /><published>2022-07-23T00:00:00+01:00</published><updated>2022-07-23T00:00:00+01:00</updated><id>http://localhost:4000/projects/huang2022emb</id><content type="html" xml:base="http://localhost:4000/projects/huang2022emb"><![CDATA[<p><img src="/assets/project/huang2022emb/overview.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<p>The goal of video activity localisation is to 
locate temporally video moments-of-interest (MoIs) 
of a specific activity described by a natural language query 
of an untrimmed continuous long video 
(often unscripted and unstructured) that contains many different activities.</p>

<p>Existing video activity localisation solutions
either adopt a proposal-free paradigm to
predict directly the start and end frames of a target moment
that align to the given query,
or a proposal-based paradigm to
generate many candidate proposals for a target moment
and aligns segment-level video features
with the query sentences.
The proposal-free methods deploy directly
the fixed manual activity endpoints labels
for model training, 
implicitly assuming these labels are well-defined.
However,
there is a considerable variation in
how activities occur in unconstrained scenarios,
<em>i.e.</em>, the manual temporal labels are
inherently uncertain and prone to significant misinformation.
On the other hand,
by formulating the localisation task as a matching problem,
the proposal-based methods
consider alignment by the whole moment
with less focus on the exact boundary matching.
Therefore,
it can be less sensitive to the boundary labels
but more reliance on salient content.
Nonetheless,
the problem of detecting accurately the start and end-point
of a target activity moment remains unsolved.</p>

<p>In this work,
we introduce <em>Elastic Moment Bounding</em> (EMB)
to address the limitation of proposal-free paradigm
by modelling explicitly label uncertainty
both in training and testing.
The key idea is that,
considering the uncertain nature of activity temporal boundary,
it is more intuitive to represent the endpoints of video activity
by temporal spans rather than specific frames.
To that end,
the EMB model conducts a proposal-based segment-wise content alignment
in addition to learning of frame-wise boundary identification.
As the predicted segment is required to be 
highly aligned with the query textual description,
we represent the gap between the predicted endpoints and 
the manually labelled endpoints as an elastic boundary
to enable optimal endpoints selection
to be consistent in semantically similar video activities.</p>

<p>Our <em>contributions</em> are: 
<strong>(1)</strong> We introduce a model to 
explore collaboratively both proposal-free and proposal-based mechanisms 
for learning to detect more accurate activity temporal boundary localisation 
when training labels are inherently uncertain. 
We formulate a new Elastic Moment Bounding (EMB) method to 
expand a manually annotated single pair of fixed activity endpoints 
to an elastic set. 
<strong>(2)</strong> To reinforce directly robust content matching 
(the spirit of proposal-based) 
as a condition to accurate endpoints localisation 
(the spirit of proposal-free) of activities in videos, 
we introduce a Guided Attention mechanism to 
explicitly optimise frame-wise boundary visual features 
subject to segment-wise content representations and vice versa.
<strong>(3)</strong> Our EMB model provides a state-of-the-art performance on 
three video activity localisation benchmark datasets, 
improving existing models that 
suffer from sensitivity to uncertainties in activity training labels.</p>

<p>Please kindly refer to the <a href="https://arxiv.org/abs/2206.12923">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><category term="open-sourced" /><category term="project-page" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">li2021loga</title><link href="http://localhost:4000/projects/li2021loga" rel="alternate" type="text/html" title="li2021loga" /><published>2021-10-26T00:00:00+01:00</published><updated>2021-10-26T00:00:00+01:00</updated><id>http://localhost:4000/projects/li2021loga</id><content type="html" xml:base="http://localhost:4000/projects/li2021loga"><![CDATA[<p><img src="/assets/project/li2021loga/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Video activity localisation by natural language 
is an important yet challenging task, 
which aims to localise temporally a video segment that 
best corresponds to a query sentence
in an untrimmed (and often unstructured) video. 
Most of the existing methods address this task
in a fully supervised manner
to learn to localise <em>moment-of-interest</em> (MoI) in videos
according to their precise start and end time indices.
Considering the high annotation cost and subjective annotation bias,
recent works focus on weakly-supervised learning without per-sentence
temporal boundary annotations in training.</p>

<p>Existing weakly-supervised solutions
localise different MoIs individually, 
which is not optimal as it neglects the fact that
the cross-sentence relations in a paragraph 
play an important role in temporally localising multiple MoIs. 
Critically, an individual sentence is sometimes ambiguous 
out of its paragraph context
and the MoIs described by a paragraph are often
semantically related to each other in their corresponding sentences.</p>

<p>In this work,
we introduce a weakly-supervised method
for video activity localisation by natural language
called <em>Cross-sentence Relations Mining</em> (CRM).
The key idea is to
explore the cross-sentence relations in a paragraph 
as constraints to better interpret and match
complex moment-wise temporal and semantic relations in videos.
Specifically,
by assuming different activities in videos are described sequentially,
we formulate a <em>temporal consistency</em> constraint to encourage
the selected moments to be temporally ordered according to their
descriptions in a paragraph.
Moreover,
we encourage moment proposal selections to satisfy cross-sentence
broader semantics in context
to minimise video-text matching ambiguities.
To that end, we introduce a <em>semantic consistency</em> constraint
to ensure that a moment selected for any pairing of two 
sentences (concatenation) in a paragraph is consistent (overlapping) with
the union of the selected segments per sentence.</p>

<p>Our <strong>contributions</strong> are:
<strong>(1)</strong>
To our best knowledge, 
this is the first idea to develop a model using <em>cross-sentence relations</em>
in a paragraph to 
explicitly represent and compute <em>cross-moment relations</em> in videos,
so as to alleviate the ambiguity of each individual sentence in video activity localisation.
<strong>(2)</strong> 
We formulate a new weakly-supervised method 
for activity localisation by natural language 
called <em>Cross-sentence Relations Mining</em> (CRM), 
that trains a model with both temporal and semantic cross-sentence relations to improve
per-sentence temporal boundary prediction in testing.
<strong>(3)</strong>
Our approach achieves the state-of-the-art performance 
on two available activity localisation benchmarks,
especially so given more complex query descriptions.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Experiments were conducted on two challenging video activity localisation benchmarks
which demonstrate the compelling multi-modal understanding ability
of CRM over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li>
    <p><strong>Charades-STA</strong> contains 12,408/3720 video-query pairs from 5338/1334 videos for training and testing, respectively. The query sentences are composed of 7.2 words on average and the average duration of the MoIs and videos are 8.1 and 30.6 seconds.</p>
  </li>
  <li>
    <p><strong>ActivityNet-Captions</strong> is a larger-scale dataset composed of 19,290 videos with 37,417/17,505/17,031 MoIs in the train/val_1/val_2 split. The average length of queries is 14 words
while that of the MoIs and untrimmed videos are 36.2 and 117.6 seconds.</p>
  </li>
</ul>

<p>Please kindly refer to the <a href="http://www.eecs.qmul.ac.uk/~sgg/papers/LiEtAl_BMVC2021.pdf">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><category term="project-page" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2021crm</title><link href="http://localhost:4000/projects/huang2021crm" rel="alternate" type="text/html" title="huang2021crm" /><published>2021-07-23T00:00:00+01:00</published><updated>2021-07-23T00:00:00+01:00</updated><id>http://localhost:4000/projects/huang2021crm</id><content type="html" xml:base="http://localhost:4000/projects/huang2021crm"><![CDATA[<p><img src="/assets/project/huang2021crm/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Video activity localisation by natural language 
is an important yet challenging task, 
which aims to localise temporally a video segment that 
best corresponds to a query sentence
in an untrimmed (and often unstructured) video. 
Most of the existing methods address this task
in a fully supervised manner
to learn to localise <em>moment-of-interest</em> (MoI) in videos
according to their precise start and end time indices.
Considering the high annotation cost and subjective annotation bias,
recent works focus on weakly-supervised learning without per-sentence
temporal boundary annotations in training.</p>

<p>Existing weakly-supervised solutions
localise different MoIs individually, 
which is not optimal as it neglects the fact that
the cross-sentence relations in a paragraph 
play an important role in temporally localising multiple MoIs. 
Critically, an individual sentence is sometimes ambiguous 
out of its paragraph context
and the MoIs described by a paragraph are often
semantically related to each other in their corresponding sentences.</p>

<p>In this work,
we introduce a weakly-supervised method
for video activity localisation by natural language
called <em>Cross-sentence Relations Mining</em> (CRM).
The key idea is to
explore the cross-sentence relations in a paragraph 
as constraints to better interpret and match
complex moment-wise temporal and semantic relations in videos.
Specifically,
by assuming different activities in videos are described sequentially,
we formulate a <em>temporal consistency</em> constraint to encourage
the selected moments to be temporally ordered according to their
descriptions in a paragraph.
Moreover,
we encourage moment proposal selections to satisfy cross-sentence
broader semantics in context
to minimise video-text matching ambiguities.
To that end, we introduce a <em>semantic consistency</em> constraint
to ensure that a moment selected for any pairing of two 
sentences (concatenation) in a paragraph is consistent (overlapping) with
the union of the selected segments per sentence.</p>

<p>Our <strong>contributions</strong> are:
<strong>(1)</strong>
To our best knowledge, 
this is the first idea to develop a model using <em>cross-sentence relations</em>
in a paragraph to 
explicitly represent and compute <em>cross-moment relations</em> in videos,
so as to alleviate the ambiguity of each individual sentence in video activity localisation.
<strong>(2)</strong> 
We formulate a new weakly-supervised method 
for activity localisation by natural language 
called <em>Cross-sentence Relations Mining</em> (CRM), 
that trains a model with both temporal and semantic cross-sentence relations to improve
per-sentence temporal boundary prediction in testing.
<strong>(3)</strong>
Our approach achieves the state-of-the-art performance 
on two available activity localisation benchmarks,
especially so given more complex query descriptions.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Experiments were conducted on two challenging video activity localisation benchmarks
which demonstrate the compelling multi-modal understanding ability
of CRM over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li>
    <p><strong>Charades-STA</strong> contains 12,408/3720 video-query pairs from 5338/1334 videos for training and testing, respectively. The query sentences are composed of 7.2 words on average and the average duration of the MoIs and videos are 8.1 and 30.6 seconds.</p>
  </li>
  <li>
    <p><strong>ActivityNet-Captions</strong> is a larger-scale dataset composed of 19,290 videos with 37,417/17,505/17,031 MoIs in the train/val_1/val_2 split. The average length of queries is 14 words
while that of the MoIs and untrimmed videos are 36.2 and 117.6 seconds.</p>
  </li>
</ul>

<p>Please kindly refer to the <a href="https://arxiv.org/abs/2107.11443">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><category term="project-page" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2020supreme</title><link href="http://localhost:4000/projects/huang2020supreme" rel="alternate" type="text/html" title="huang2020supreme" /><published>2020-07-23T00:00:00+01:00</published><updated>2020-07-23T00:00:00+01:00</updated><id>http://localhost:4000/projects/huang2020supreme</id><content type="html" xml:base="http://localhost:4000/projects/huang2020supreme"><![CDATA[<p><img src="/assets/project/huang2020supreme/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Video activity localisation by natural language 
is an important yet challenging task, 
which aims to localise temporally a video segment that 
best corresponds to a query sentence
in an untrimmed (and often unstructured) video. 
Most of the existing methods address this task
in a fully supervised manner
to learn to localise <em>moment-of-interest</em> (MoI) in videos
according to their precise start and end time indices.
Considering the high annotation cost and subjective annotation bias,
recent works focus on weakly-supervised learning without per-sentence
temporal boundary annotations in training.</p>

<p>Existing weakly-supervised solutions
localise different MoIs individually, 
which is not optimal as it neglects the fact that
the cross-sentence relations in a paragraph 
play an important role in temporally localising multiple MoIs. 
Critically, an individual sentence is sometimes ambiguous 
out of its paragraph context
and the MoIs described by a paragraph are often
semantically related to each other in their corresponding sentences.</p>

<p>In this work,
we introduce a weakly-supervised method
for video activity localisation by natural language
called <em>Cross-sentence Relations Mining</em> (CRM).
The key idea is to
explore the cross-sentence relations in a paragraph 
as constraints to better interpret and match
complex moment-wise temporal and semantic relations in videos.
Specifically,
by assuming different activities in videos are described sequentially,
we formulate a <em>temporal consistency</em> constraint to encourage
the selected moments to be temporally ordered according to their
descriptions in a paragraph.
Moreover,
we encourage moment proposal selections to satisfy cross-sentence
broader semantics in context
to minimise video-text matching ambiguities.
To that end, we introduce a <em>semantic consistency</em> constraint
to ensure that a moment selected for any pairing of two 
sentences (concatenation) in a paragraph is consistent (overlapping) with
the union of the selected segments per sentence.</p>

<p>Our <strong>contributions</strong> are:
<strong>(1)</strong>
To our best knowledge, 
this is the first idea to develop a model using <em>cross-sentence relations</em>
in a paragraph to 
explicitly represent and compute <em>cross-moment relations</em> in videos,
so as to alleviate the ambiguity of each individual sentence in video activity localisation.
<strong>(2)</strong> 
We formulate a new weakly-supervised method 
for activity localisation by natural language 
called <em>Cross-sentence Relations Mining</em> (CRM), 
that trains a model with both temporal and semantic cross-sentence relations to improve
per-sentence temporal boundary prediction in testing.
<strong>(3)</strong>
Our approach achieves the state-of-the-art performance 
on two available activity localisation benchmarks,
especially so given more complex query descriptions.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Experiments were conducted on two challenging video activity localisation benchmarks
which demonstrate the compelling multi-modal understanding ability
of CRM over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li>
    <p><strong>Charades-STA</strong> contains 12,408/3720 video-query pairs from 5338/1334 videos for training and testing, respectively. The query sentences are composed of 7.2 words on average and the average duration of the MoIs and videos are 8.1 and 30.6 seconds.</p>
  </li>
  <li>
    <p><strong>ActivityNet-Captions</strong> is a larger-scale dataset composed of 19,290 videos with 37,417/17,505/17,031 MoIs in the train/val_1/val_2 split. The average length of queries is 14 words
while that of the MoIs and untrimmed videos are 36.2 and 117.6 seconds.</p>
  </li>
</ul>

<p>Please kindly refer to the <a href="https://arxiv.org/abs/2006.04737">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="preprint" /><category term="public" /><category term="uploaded" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2020pica</title><link href="http://localhost:4000/projects/huang2020pica" rel="alternate" type="text/html" title="huang2020pica" /><published>2020-03-05T00:00:00+00:00</published><updated>2020-03-05T00:00:00+00:00</updated><id>http://localhost:4000/projects/huang2020pica</id><content type="html" xml:base="http://localhost:4000/projects/huang2020pica"><![CDATA[<p><img src="/assets/project/huang2020pica/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Deep Clustering, which jointly optimises the objectives of representation learning and clustering with the help of deep learning techniques, is proposed to address the limitation of traditional cluster analysis algorithms when dealing with high-dimensional imagery data with indiscriminative visual representations. Although conducting cluster analysis with learnable representations holds the potential to benefit clustering on unlabelled data, how to improve the semantic plausibility of these clusters remains an open problem.</p>

<p>Recent deep clustering models either iteratively estimate cluster assignment and/or inter-sample relations which are then used as hypotheses in supervising the learning of deep neural networks, or used in conjunction with cluster constraints. The alternate training strategy is susceptible to error-propagation due to inaccurate membership estimation. The simultaneous one, which usually supervised by pretext tasks that require good cluster structure, suffers from the vague connection between training supervision and cluster objectives. Without global solution-level guidance to select from all the possible separations, the resulted clusters tend to be semantically less plausible.</p>

<p>In this work, we propose a deep clustering method called <em>PartItion Confidence mAximisation</em> (PICA). Due to the high visual similarity shared by samples from the same semantic classes, assigning them into different clusters will reduce the resulted intra-cluster compactness and inter-cluster diversity, <em>i.e.</em> lower partition confidence. Based on this insight, PICA is designed to encourage the model to learn the most <em>confident</em> clusters from all the possible solutions in order to find the most semantically plausible inter-class separation. This is in spirit of traditional maximal margin clustering which also seeks for most separable clustering solutions with shallow models (<em>e.g.</em> SVM), but differs notably in that both the feature representations and decision boundaries are end-to-end learned in our deep learning model. Specifically, a partition uncertainty index (PUI) is proposed to quantifies how confidently a deep model can make sense and separate a set of target images. A stochastic approximation of PUI is introduced to enable standard mini-batch based learning and a novel objective loss function is formulated for training with any off-the-shelf networks.</p>

<p>Our <em>contributions</em> are threefold:</p>
<ol>
  <li>We propose the idea of learning the most semantically plausible clustering solution by maximising partition confidence, which extends the classical maximal margin clustering idea to the deep learning paradigm. 
<!-- The proposed method makes no strong hypothesis on local inter-sample relations and/or cluster assignment which usually leads to error-propagation and inferior clustering solutions.  --></li>
  <li>We introduce a novel deep clustering method, called <em>PartItion Confidence mAximisation</em> (PICA) which is built upon a newly introduced partition uncertainty index that is designed elegantly to quantify the global confidence of the clustering solution. 
<!-- To enable formulating a deep learning objective loss function, a novel transformation of the partition uncertainty index is further proposed. PICA can be trained end-to-end using a single objective loss function without whistles and bells (*e.g.* complex multi-stage alternation and multiple loss functions) to simultaneously learn a deep neural network and cluster assignment that can be mapped to the semantic category one-to-one. --></li>
  <li>A stochastic approximation of the partition uncertainty index is formulated to decouple it from the whole set of target images, therefore, enabling a ready adoption of the standard mini-batch model training.</li>
</ol>

<h2 id="benchmarks">Benchmarks</h2>
<p>Extensive experiments are conducted on six challenging objects recognition benchmarks which demonstrates the advantages of PICA over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li><strong>CIFAR10(/100)</strong>: A natural image dataset with 50,000/10,000 samples from 10(/100) classes for training and testing respectively.</li>
  <li><strong>STL10</strong>: An ImageNet sourced dataset containing 500/800 training/test images from each of 10 classes and additional 100,000 samples from several unknown categories.</li>
  <li><strong>ImageNet-10 and ImageNet-Dogs</strong>:
Two subsets of ImageNet: the former with 10 random selected subjects and the latter with 15 dog breeds.</li>
  <li><strong>Tiny-ImageNet</strong>: A subset of ImageNet with 200 classes. There are 100,000/10,000 training/test images evenly distributed in each category.</li>
</ul>

<p>Please kindly refer to the <a href="/assets/project/huang2020pica/paper.pdf">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><category term="project-page" /><category term="open-sourced" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2020pad</title><link href="http://localhost:4000/projects/huang2020pad" rel="alternate" type="text/html" title="huang2020pad" /><published>2019-12-11T00:00:00+00:00</published><updated>2019-12-11T00:00:00+00:00</updated><id>http://localhost:4000/projects/huang2020pad</id><content type="html" xml:base="http://localhost:4000/projects/huang2020pad"><![CDATA[<p><img src="/assets/project/huang2020pad/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>
<p>Convolutional neural networks (CNNs) trained in a supervised fashion have significantly boosted the state-of-the-art performance in computer vision. Moreover, the feature representations of a supervised CNN (e.g. trained for classification on ImageNet) generalise to new tasks. Despite such remarkable success, this approach is limited due to a number of stringent assumptions that do not always hold valid. Due to the only need for access of unlabelled data typically available at scale, unsupervised deep learning provides a conceptually generic and scalable solution to these limitations.</p>

<p>One intuitive strategy for unsupervised deep learning is joint learning of feature representations and data clustering. This objective is extremely hard due to the numerous combinatorial configurations of unlabelled data alongside highly complex inter-class decision boundaries. To avoid clustering errors as well as the following propagation, instance learning is proposed whereby every single sample is treated as an independent class. However, this simplified supervision is often rather ambiguous particularly around class centres, therefore, leading to weak class discrimination. As an intermediate representation, tiny neighbourhoods are leveraged for preserving the advantages of both data
clustering and instance learning. But this method is restricted by the small size of local neighbourhoods.</p>

<p>In this work, we aim to solve the algorithmic limitations of existing unsupervised deep learning methods. To that end, we propose a general-purpose <em>Progressive Affinity Diffusion</em> (PAD) method for training unsupervised models. Requiring no prior knowledge of class number, PAD performs
model-matureness-adaptive data group inference in training for more reliably revealing the underlying sample-to-class memberships.</p>

<p>We make three contributions:</p>
<ol>
  <li>We propose a novel idea of leveraging strongly connected subgraphs as a self-supervision structure for more reliable unsupervised deep learning.</li>
  <li>We formulate a <em>Progressive Affinity Diffusion</em> (PAD) method for modelmatureness-adaptive discovery of strongly connected subgraphs during training through affinity diffusion across adjacent neighbourhoods.</li>
  <li>We design a group structure aware objective loss formulation for more discriminative capitalising of strongly connected subgraphs in model representation learning.</li>
</ol>

<h2 id="benchmarks">Benchmarks</h2>
<p>Extensive experiments are conducted on both image classification and cluster analysis using the following six datasets and the results show the advantages of our PAD method over a wide variety of existing state-of-the-art approaches.</p>
<ul>
  <li><strong>CIFAR10(/100)</strong>: An image dataset with 50,000/10,000 train/test images from 10 (/100) object classes.Each class has 6,000 (/600) images with size \(32\!\times\!32\).</li>
  <li><strong>SVHN</strong>: A Street View House Numbers dataset including 10 classes of digit images.</li>
  <li><strong>ImageNet</strong>: A large 1,000 classes object dataset with 1.2 million images for training and 50,000 for test.</li>
  <li><strong>MNIST</strong>: A hand-written digits dataset with 60,000/10,000 train/test images from 10 digit classes.</li>
  <li><strong>STL10</strong>: An ImageNet adapted dataset containing 500/800 train/test samples from 10 classes as well as 100,000 unlabelled images from auxiliary unknown classes.</li>
</ul>

<p>Please kindly refer to the <a href="/assets/project/huang2020pad/paper.pdf">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="uploaded" /><category term="project-page" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">huang2019and</title><link href="http://localhost:4000/projects/huang2019and" rel="alternate" type="text/html" title="huang2019and" /><published>2019-04-25T00:00:00+01:00</published><updated>2019-04-25T00:00:00+01:00</updated><id>http://localhost:4000/projects/huang2019and</id><content type="html" xml:base="http://localhost:4000/projects/huang2019and"><![CDATA[<p><img src="/assets/project/huang2019and/training-pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>
<p>Deep neural networks have significantly advanced the progress of computer vision problems, nevertheless, most of them are heavily relying on massive collection of exhaustively labelled training data. As such, unsupervised learning of deep features has recently drawn increasing attention.</p>

<p>In the literature, representative unsupervised deep learning methods include clustering and sample specificity analysis. Methods fall in the former category has great potential with the best case reaching the performance of supervised learning but is error-prone. In contrast, sample specificity learning treats every single sample as an independent class and hypothesis that the model can reveal the underlying class-to-class semantic similarity structure. However, the ambiguous supervision leads to its weak discriminative ability. Other contemporary methods like self-supervised learning and data synthesis share a similar limitation due to the insufficient correlation between the auxiliary supervision and the underlying class target.</p>

<p>We present a generic unsupervised deep learning method called <em>Anchor Neighbourhood Discovery</em> (AND). With a <em>divide-and-conquer</em> principle, the AND discovers class consistent neighbourhoods anchored to individual training samples (<em>divide</em>) and propagates the local inter-sample class relationships within such neighbourhoods (<em>conquer</em>) for more reliably extracting the latent discrimination information during model training.</p>

<p>We make three contributions:</p>
<ol>
  <li>We propose the idea of exploiting local neighbourhoods for unsupervised deep learning. To our best knowledge, it is the first attempt at exploring the concept of neighbourhood for end-to-end unsupervised deep learning of visual features.</li>
  <li>We formulate an <em>Anchor Neighbourhood Discovery</em> (AND) approach to progressive unsupervised deep learning.</li>
  <li>We further introduce a curriculum learning algorithm to gradually perform neighbourhood discovery for maximising the class consistency of neighbourhoods therefore enhancing the unsupervised learning capability.</li>
</ol>

<h2 id="benchmarks">Benchmarks</h2>
<p>Extensive experiments are conducted on the following six datasets and the results show the advantages of our AND method over a wide variety of existing state-of-the-art unsupervised deep learning models.</p>
<ul>
  <li><strong>CIFAR10(/100)</strong>: An image dataset with 50,000/10,000 train/test images from 10 (/100) object classes.Each class has 6,000 (/600) images with size \(32\!\times\!32\).</li>
  <li><strong>SVHN</strong>: A Street View House Numbers dataset including 10 classes of digit images.</li>
  <li><strong>ImageNet</strong>: A large 1,000 classes object dataset with 1.2 million images for training and 50,000 for test.</li>
  <li><strong>CUB200-2011</strong>: A fine-grained dataset containing 5,994/5,794 train/test images of 200 bird species.</li>
  <li><strong>Stanford Dogs</strong>: A fine-grained dataset with 12,000/8,580 train/test images of 120 dog breeds.</li>
</ul>

<p>Please kindly refer to the <a href="https://arxiv.org/abs/1904.11567">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>]]></content><author><name>{&quot;firstname&quot;=&gt;&quot;Jiabo&quot;, &quot;lastname&quot;=&gt;&quot;Huang&quot;, &quot;nickname&quot;=&gt;&quot;Raymond&quot;, &quot;email&quot;=&gt;&quot;jiabo.huang AT qmul.ac.uk&quot;, &quot;phone&quot;=&gt;&quot;+0044 07561775861&quot;, &quot;affliations&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Computer Vision Group&quot;, &quot;link&quot;=&gt;&quot;http://www.dcs.qmul.ac.uk/research/vision/&quot;}, {&quot;name&quot;=&gt;&quot;School of Electronic Engineering and Computer Science&quot;, &quot;link&quot;=&gt;&quot;http://www.eecs.qmul.ac.uk/&quot;}, {&quot;name&quot;=&gt;&quot;Queen Mary University of London&quot;, &quot;link&quot;=&gt;&quot;http://www.qmul.ac.uk/&quot;, &quot;address&quot;=&gt;[{&quot;line&quot;=&gt;&quot;Mile End Road&quot;}, {&quot;line&quot;=&gt;&quot;London, E1 4NS, UK&quot;}]}], &quot;thumb&quot;=&gt;&quot;asserts/home/thumb.jpg&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;Github&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-github&quot;, &quot;link&quot;=&gt;&quot;https://github.com/Raymond-sci&quot;}, {&quot;name&quot;=&gt;&quot;Google Scholar&quot;, &quot;icon&quot;=&gt;&quot;fa-brands fa-google&quot;, &quot;link&quot;=&gt;&quot;https://scholar.google.com/citations?user=o_ecsPcAAAAJ&amp;hl=en&quot;}]}</name><email>jiabo.huang AT qmul.ac.uk</email></author><category term="project" /><category term="paper" /><category term="accepted" /><category term="public" /><category term="open-sourced" /><category term="project-page" /><category term="uploaded" /><summary type="html"><![CDATA[]]></summary></entry></feed>