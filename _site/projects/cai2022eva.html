<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0055)https://qmul-survface.github.io/QMUL-SurvFace/index.htm -->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>cai2022eva</title>
    <meta name="description" content="">
    <meta name="keywords" content="">
    <link href="/assets/project/style.css" rel="stylesheet">
</head>
<body>
  <div id="content">
    <div id="content-inner">
      <h1 class="title">Hybrid-Learning Video Moment Retrieval across Multi-Domain Labels</h1>
      
      <p class="center">
        <em>Accepted by British Machine Vision Conference (BMVC'22)</em>
      </p>
      
      <p class="authors">
      
        
          <a href="https://lvgd.github.io/">Weitong Cai</a>
        
        
        <space></space>
        
      
        
          <b><a href="http://www.eecs.qmul.ac.uk/~jh327/">Jiabo Huang</a></b>
        
        
        <space></space>
        
      
        
          <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
        
        
      
      </p>
      <p class="affiliations">
      
        <a href="http://vision.eecs.qmul.ac.uk/">Queen Mary University of London</a>
        
      
      </p>
      <p><img src="/assets/project/cai2022eva/pipeline.jpg" alt="cover" width="90%" class="center" />
<!--*Figure 1. Overview of the proposed Anchor Neighbourhood Discovery (AND) method for unsupervised deep learning.*{:.center}--></p>

<h2 id="description">Description</h2>

<p>Video activity localisation by natural language 
is an important yet challenging task, 
which aims to localise temporally a video segment that 
best corresponds to a query sentence
in an untrimmed (and often unstructured) video. 
Most of the existing methods address this task
in a fully supervised manner
to learn to localise <em>moment-of-interest</em> (MoI) in videos
according to their precise start and end time indices.
Considering the high annotation cost and subjective annotation bias,
recent works focus on weakly-supervised learning without per-sentence
temporal boundary annotations in training.</p>

<p>Existing weakly-supervised solutions
localise different MoIs individually, 
which is not optimal as it neglects the fact that
the cross-sentence relations in a paragraph 
play an important role in temporally localising multiple MoIs. 
Critically, an individual sentence is sometimes ambiguous 
out of its paragraph context
and the MoIs described by a paragraph are often
semantically related to each other in their corresponding sentences.</p>

<p>In this work,
we introduce a weakly-supervised method
for video activity localisation by natural language
called <em>Cross-sentence Relations Mining</em> (CRM).
The key idea is to
explore the cross-sentence relations in a paragraph 
as constraints to better interpret and match
complex moment-wise temporal and semantic relations in videos.
Specifically,
by assuming different activities in videos are described sequentially,
we formulate a <em>temporal consistency</em> constraint to encourage
the selected moments to be temporally ordered according to their
descriptions in a paragraph.
Moreover,
we encourage moment proposal selections to satisfy cross-sentence
broader semantics in context
to minimise video-text matching ambiguities.
To that end, we introduce a <em>semantic consistency</em> constraint
to ensure that a moment selected for any pairing of two 
sentences (concatenation) in a paragraph is consistent (overlapping) with
the union of the selected segments per sentence.</p>

<p>Our <strong>contributions</strong> are:
<strong>(1)</strong>
To our best knowledge, 
this is the first idea to develop a model using <em>cross-sentence relations</em>
in a paragraph to 
explicitly represent and compute <em>cross-moment relations</em> in videos,
so as to alleviate the ambiguity of each individual sentence in video activity localisation.
<strong>(2)</strong> 
We formulate a new weakly-supervised method 
for activity localisation by natural language 
called <em>Cross-sentence Relations Mining</em> (CRM), 
that trains a model with both temporal and semantic cross-sentence relations to improve
per-sentence temporal boundary prediction in testing.
<strong>(3)</strong>
Our approach achieves the state-of-the-art performance 
on two available activity localisation benchmarks,
especially so given more complex query descriptions.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Experiments were conducted on two challenging video activity localisation benchmarks
which demonstrate the compelling multi-modal understanding ability
of CRM over a wide range of the state-of-the-art approaches.</p>
<ul>
  <li>
    <p><strong>Charades-STA</strong> contains 12,408/3720 video-query pairs from 5338/1334 videos for training and testing, respectively. The query sentences are composed of 7.2 words on average and the average duration of the MoIs and videos are 8.1 and 30.6 seconds.</p>
  </li>
  <li>
    <p><strong>ActivityNet-Captions</strong> is a larger-scale dataset composed of 19,290 videos with 37,417/17,505/17,031 MoIs in the train/val_1/val_2 split. The average length of queries is 14 words
while that of the MoIs and untrimmed videos are 36.2 and 117.6 seconds.</p>
  </li>
</ul>

<p>Please kindly refer to the <a href="http://www.eecs.qmul.ac.uk/~sgg/papers/CaiEtAl_BMVC2022.pdf">paper</a> for more details and feel free to reach <a href="http://localhost:4000">me</a> for any question.</p>

      <!-- <p role="update_time"><small>Last updated at 2022.10.08</small></p> -->
    </div>
  </div>
</body>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</html>
